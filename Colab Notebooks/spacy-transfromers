{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spacy-transfromers","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CP6zGVsox13p","colab_type":"code","colab":{}},"source":["! pip install spacy-transformers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oxl5lWyTyC5p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"outputId":"8113c0af-c04f-480d-fc98-8c5e23c4f570","executionInfo":{"status":"ok","timestamp":1573816773250,"user_tz":-330,"elapsed":19637,"user":{"displayName":"Vimarsh Chaturvedi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA5T0kFhYT-TWqP6SnmS0s4ZarLWqT4RtqKIvBxxA=s64","userId":"08007523027797049436"}}},"source":["! python -m spacy download en_trf_bertbaseuncased_lg"],"execution_count":2,"outputs":[{"output_type":"stream","text":["  Building wheel for en-trf-bertbaseuncased-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-trf-bertbaseuncased-lg: filename=en_trf_bertbaseuncased_lg-2.2.0-cp36-none-any.whl size=405819945 sha256=74828b8d778d90ac82cb8a1f4534cb73fab751195d9a53c3dbd4bde51f3a6758\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-chtdyi0q/wheels/f6/60/8c/c6f517ef9729972f1be15c3aab4b93e7ec9fbeb71d072a84de\n","Successfully built en-trf-bertbaseuncased-lg\n","Installing collected packages: en-trf-bertbaseuncased-lg\n","Successfully installed en-trf-bertbaseuncased-lg-2.2.0\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_trf_bertbaseuncased_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqWFpP9PyFE-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"20e2ee3f-b8ab-47bc-b685-fd9b88b32f20","executionInfo":{"status":"ok","timestamp":1573816780372,"user_tz":-330,"elapsed":106544,"user":{"displayName":"Vimarsh Chaturvedi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA5T0kFhYT-TWqP6SnmS0s4ZarLWqT4RtqKIvBxxA=s64","userId":"08007523027797049436"}}},"source":["import plac\n","import re\n","import random\n","import json\n","from pathlib import Path\n","from collections import Counter\n","import thinc.extra.datasets\n","import spacy\n","import torch\n","from spacy.util import minibatch\n","import tqdm\n","import unicodedata\n","import wasabi\n","from spacy_transformers.util import cyclic_triangular_rate"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"mWDWSTD3yKWa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMtRAylFySsh","colab_type":"code","colab":{}},"source":["def read_inputs(input_path):\n","    texts = []\n","    cats = []\n","    with input_path.open(mode=\"r\") as file_:\n","        for line in file_:\n","            text, gold = json.loads(line)\n","            text = preprocess_text(text)\n","            texts.append(text)\n","            cats.append(gold[\"cats\"])\n","    return texts, cats\n","\n","\n","def make_sentence_examples(nlp, texts, labels):\n","    \"\"\"Treat each sentence of the document as an instance, using the doc labels.\"\"\"\n","    sents = []\n","    sent_cats = []\n","    for text, cats in zip(texts, labels):\n","        doc = nlp.make_doc(text)\n","        doc = nlp.get_pipe(\"sentencizer\")(doc)\n","        for sent in doc.sents:\n","            sents.append(sent.text)\n","            sent_cats.append(cats)\n","    return sents, sent_cats\n","\n","\n","white_re = re.compile(r\"\\s\\s+\")\n","\n","\n","def preprocess_text(text):\n","    text = text.replace(\"<s>\", \"<open-s-tag>\")\n","    text = text.replace(\"</s>\", \"<close-s-tag>\")\n","    text = white_re.sub(\" \", text).strip()\n","    return \"\".join(\n","        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n","    )\n","\n","\n","def load_data(*, limit=0, dev_size=2000):\n","    \"\"\"Load data from the IMDB dataset, splitting off a held-out set.\"\"\"\n","    if limit != 0:\n","        limit += dev_size\n","    assert dev_size != 0\n","    train_data, _ = thinc.extra.datasets.imdb(limit=limit)\n","    assert len(train_data) > dev_size\n","    random.shuffle(train_data)\n","    dev_data = train_data[:dev_size]\n","    train_data = train_data[dev_size:]\n","    train_texts, train_labels = _prepare_partition(train_data, preprocess=False)\n","    dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=False)\n","    return (train_texts, train_labels), (dev_texts, dev_labels)\n","\n","\n","def load_data_for_final_test(*, limit=0):\n","    print(\n","        \"Warning: Using test data. You should use development data for most experiments.\"\n","    )\n","    train_data, test_data = thinc.extra.datasets.imdb()\n","    random.shuffle(train_data)\n","    train_data = train_data[-limit:]\n","    train_texts, train_labels = _prepare_partition(train_data)\n","    test_texts, test_labels = _prepare_partition(test_data)\n","    return (train_texts, train_labels), (test_texts, test_labels)\n","\n","\n","def _prepare_partition(text_label_tuples, *, preprocess=False):\n","    texts, labels = zip(*text_label_tuples)\n","    if preprocess:\n","        # Preprocessing can mask errors in our handling of noisy text, so\n","        # we don't want to do it by default\n","        texts = [preprocess_text(text) for text in texts]\n","    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n","    return texts, cats\n","\n","\n","def evaluate(nlp, texts, cats, pos_label):\n","    tp = 0.0  # True positives\n","    fp = 0.0  # False positives\n","    fn = 0.0  # False negatives\n","    tn = 0.0  # True negatives\n","    total_words = sum(len(text.split()) for text in texts)\n","    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n","        for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n","            gold = cats[i]\n","            for label, score in doc.cats.items():\n","                if label not in gold:\n","                    continue\n","                if label != pos_label:\n","                    continue\n","                if score >= 0.5 and gold[label] >= 0.5:\n","                    tp += 1.0\n","                elif score >= 0.5 and gold[label] < 0.5:\n","                    fp += 1.0\n","                elif score < 0.5 and gold[label] < 0.5:\n","                    tn += 1\n","                elif score < 0.5 and gold[label] >= 0.5:\n","                    fn += 1\n","            pbar.update(len(doc.text.split()))\n","    precision = tp / (tp + fp + 1e-8)\n","    recall = tp / (tp + fn + 1e-8)\n","    if (precision + recall) == 0:\n","        f_score = 0.0\n","    else:\n","        f_score = 2 * (precision * recall) / (precision + recall)\n","    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKRyloLayTSN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}